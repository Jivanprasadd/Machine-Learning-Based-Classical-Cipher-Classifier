{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxtZzYcOReSI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def add_random_noise(texts, noise_level=0.1):\n",
        "    noisy_texts = []\n",
        "    for text in texts:\n",
        "        noisy_text = ''.join([c if np.random.rand() > noise_level else chr(ord(c) + np.random.randint(-3, 3)) for c in text])\n",
        "        noisy_texts.append(noisy_text)\n",
        "    return noisy_texts\n",
        "\n",
        "training_data = pd.read_csv('Dataset.csv')\n",
        "\n",
        "cipher_texts_augmented = add_random_noise(training_data.iloc[:, 0].values)\n",
        "cipher_texts_combined = np.concatenate((training_data.iloc[:, 0].values, cipher_texts_augmented))\n",
        "labels_text_combined = np.concatenate((training_data.iloc[:, 1].values, training_data.iloc[:, 1].values))\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(cipher_texts_combined)\n",
        "cipher_texts_tokenized = tokenizer.texts_to_sequences(cipher_texts_combined)\n",
        "max_sequence_length = 40\n",
        "cipher_texts_padded = pad_sequences(cipher_texts_tokenized, maxlen=max_sequence_length)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels_numeric = label_encoder.fit_transform(labels_text_combined)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(cipher_texts_padded, labels_numeric, test_size=0.2, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "def create_model(units_lstm=128, dropout_rate=0.5):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
        "    model.add(Bidirectional(LSTM(units=units_lstm, return_sequences=True)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Bidirectional(LSTM(units=units_lstm, return_sequences=True)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Bidirectional(LSTM(units=units_lstm)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = create_model(units_lstm=128, dropout_rate=0.3)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=2, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "model_filename = 'AdvancedCiphers.h5'\n",
        "model.save(model_filename)\n",
        "print(\"Trained model saved to:\", model_filename)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "print(\"Test Set Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_labels))\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "tokenizer_filename = 'Tokenizer.json'\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "\n",
        "with open(tokenizer_filename, 'w') as f:\n",
        "    f.write(tokenizer_json)\n",
        "\n",
        "print(\"Tokenizer saved to:\", tokenizer_filename)\n",
        "\n",
        "label_encoder_filename = 'LabelEncoder.npy'\n",
        "np.save(label_encoder_filename, label_encoder.classes_)\n",
        "\n",
        "print(\"Label encoder saved to:\", label_encoder_filename)\n",
        "\n",
        "num_texts = int(input(\"Enter the number of cipher texts you want to test: \"))\n",
        "for i in range(num_texts):\n",
        "    user_input = input(\"Enter cipher text {} for testing: \".format(i + 1))\n",
        "\n",
        "    user_input = user_input.lower()\n",
        "    user_input_tokenized = tokenizer.texts_to_sequences([user_input])\n",
        "    user_input_padded = pad_sequences(user_input_tokenized, maxlen=max_sequence_length)\n",
        "\n",
        "    prediction_one_hot = model.predict(user_input_padded)\n",
        "    predicted_label_numeric = np.argmax(prediction_one_hot, axis=1)\n",
        "    predicted_label = label_encoder.inverse_transform(predicted_label_numeric)[0]\n",
        "\n",
        "    print(\"Predicted Label for input {}: {}\".format(i + 1, predicted_label))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}